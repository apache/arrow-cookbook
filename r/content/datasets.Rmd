# Manipulating Data - Datasets

## Introduction

While Arrow allows you to work with single files imported as Tables, you can 
also work with Datasets.  These differ from tables as they can be split across 
multiple files/folders via partitioning, which means that you avoid problems 
associated with storing all your data in a single file.  This can provide 
further advantages to using Arrow, as you can now conduct analyses which only 
read in the necessary files.

It's possible to have datasets in Parquet, Feather (aka Arrow), and CSV (or 
other text-delimited formats).  If you are choosing a dataset format, we 
recommend Parquet or Feather, which will likely lead to improved performance 
when compared to CSVs due to their capaibilties around metadata and compression.

## Write a dataset to disk

You want to write a dataset to disk in a single Parquet file.

### Solution

```{r, write_dataset_basic}
library(dplyr)
write_dataset(dataset = starwars, path = "starwars_data")
```

```{r, test_write_dataset_basic, opts.label = "test"}

test_that("write_dataset_basic works as expected", {
  
  expect_true(file.exists("starwars_data"))
  expect_length(list.files("starwars_data"), 1)
  
})

```

### Discussion

The default behaviour is to write a single Parquet file.

## Write a dataset to disk - multiple files

You want to write a dataset to disk partitioned across multiple files.

### Solution

```{r, write_dataset_partitioned}
write_dataset(dataset = group_by(starwars, homeworld), path = "starwars_data_partitioned")
```

```{r, test_write_dataset_partitioned, opts.label = "test"}

test_that("write_dataset_partitioned works as expected", {
  
  expect_true(file.exists("starwars_data_partitioned"))
  expect_length(list.files("starwars_data_partitioned", recursive = TRUE), 49)
  
})

```

### Discussion

The data is written to separate folders based on the values in the `homeworld` 
column.  The default behaviour is to use Hive-style (i.e. "col_name=value" folder names)
partitions.

```{r}
# Take a look at the files in this directory
list.files("starwars_data_partitioned", recursive = TRUE)
```

Note that in the example above, when there was an `NA` value in the `homeworld`
column, these values are written to the `homeworld=__HIVE_DEFAULT_PARTITION__`
directory.

You can specify multiple grouping variables to add extra layers of partitioning.

```{r, write_dataset_partitioned_deeper}
write_dataset(
  dataset = group_by(starwars, homeworld, species),
  path = "starwars_partitioned_twice"
)
```

```{r, test_write_dataset_partitioned_deeper, opts.label = "test"}

test_that("write_dataset_partitioned_deeper works as expected", {
  
  expect_true(file.exists("starwars_partitioned_twice"))
  expect_length(list.files("starwars_partitioned_twice", recursive = TRUE), 58)
  
})

```


```{r}
# Take a look at the files in this directory
list.files("starwars_partitioned_twice", recursive = TRUE)
```

## Write a dataset to disk - Feather format

You want to write a dataset to disk in a single Feather file.

### Solution

```{r, write_dataset_feather}
library(dplyr)
write_dataset(dataset = starwars, path = "starwars_data_feather", format = "feather")
```

```{r, test_write_dataset_feather, opts.label = "test"}

test_that("write_dataset_feather works as expected", {
  
  expect_true(file.exists("starwars_data_feather"))
  expect_length(list.files("starwars_data_feather"), 1)
  
})

```

## Write a dataset to disk - CSV format

You want to write a dataset to disk in a single CSV file.

### Solution

```{r, write_dataset_csv}
library(dplyr)
write_dataset(dataset = starwars, path = "starwars_data_csv", format = "csv")
```

```{r, test_write_dataset_feather, opts.label = "test"}

test_that("write_dataset_csv works as expected", {
  
  expect_true(file.exists("starwars_data_csv"))
  expect_length(list.files("starwars_data_csv"), 1)
  
})

```
